# Time-Series-Prediction
# -*- coding: utf-8 -*-
"""Bilyoner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12bVt6QNTXS4dcjif7R0ItBev7sphAepY
"""

import pywt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#import pandas_datareader
from pandas_datareader import data as pdr
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn import model_selection, tree, preprocessing, metrics, linear_model
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import keras
import tensorflow as tf
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.models import Sequential
from keras.layers import LSTM, Dense,Dropout,RepeatVector, TimeDistributed, SimpleRNN
from keras.models import load_model
from math import sqrt
from pandas import read_csv
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.layers import Conv1D, MaxPooling1D, Flatten, ConvLSTM2D
from keras_self_attention import SeqSelfAttention
import keras.backend as K
from keras.wrappers.scikit_learn import KerasRegressor
from scipy.stats.stats import pearsonr
from keras.callbacks import LearningRateScheduler
from keras.callbacks import History
from statsmodels.tsa.stattools import grangercausalitytests
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from pandas.plotting import register_matplotlib_converters
from statsmodels.tsa.api import VAR
from pandas_datareader import data as pdr
from statsmodels.tsa.stattools import kpss
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.vector_ar.vecm import coint_johansen
from statsmodels.stats.stattools import durbin_watson
import warnings
from statsmodels.tools.sm_exceptions import InterpolationWarning
warnings.simplefilter('ignore', InterpolationWarning)
##

def plot_series(time, series, format="-", start=0, end=None):
    #plt.figure(figsize=(12,8))
    plt.plot(time[start:end], series[start:end], format)
    #plt.yticks([0, 1, 2], ['January', 'February', 'March'])
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.grid(True)
def split_dataset(data):
    # split into standard weeks
    train, test = data[1:-len(x_train)], data[len(x_train):-6]
    train = np.array(np.split(train, float(len(train))/7))
    test = np.array(np.split(test, float(len(test)) / 7))
    return train, test
def to_supervised(train, n_input, n_out):
    # flatten data
    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))
    X, y = list(), list()
    in_start = 0
    # step over the entire history one time step at a time
    for _ in range(len(data)):
    # define the end of the input sequence
        in_end = in_start + n_input
        out_end = in_end + n_out
        # ensure we have enough data for this instance
        if out_end < len(data):
            x_input = data[in_start:in_end, 0]
            x_input = x_input.reshape((len(x_input), 1))
            X.append(x_input)
            y.append(data[in_end:out_end, 0])
        # move along one time step
        in_start += 1
    return np.array(X), np.array(y)
def build_model(train, n_input):
    # prepare data
    train_x, train_y = to_supervised(train, n_input,n_out)
    # define parameters
    verbose, epochs, batch_size = 1, 10, 16
    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]
    # reshape output into [samples, timesteps, features]
    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))
    # define model
    model = Sequential()
    model.add(LSTM(10, activation= 'relu' , input_shape=(n_timesteps, n_features)))
    model.add(RepeatVector(n_outputs))
    model.add(LSTM(10, activation= 'relu' , return_sequences=True))
    model.add(TimeDistributed(Dense(10, activation= 'relu' )))
    model.add(TimeDistributed(Dense(1)))
    model.compile(loss= 'mse' , optimizer= 'adam' )
    # fit network
    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)
    return model
def not_build_model(train, n_input):
# prepare data
    train_x, train_y = to_supervised(train, n_input,n_out)
    # define parameters
    verbose, epochs, batch_size = 1, 10, 16
    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]
    # define model
    model = Sequential()
    model.add(LSTM(10, activation= 'relu' , input_shape=(n_timesteps, n_features)))
    model.add(Dense(10, activation= 'relu' ))
    model.add(Dense(n_outputs))
    model.compile(loss= 'mse' , optimizer= 'adam' )
    # fit network
    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)
    return model
def evaluate_forecasts(actual, predicted):
    scores = list()
# calculate an RMSE score for each day
    for i in range(actual.shape[1]):
    # calculate mse
        mse = mean_squared_error(actual[:, i], predicted[:, i])
        # calculate rmse
        rmse = np.sqrt(mse)
        # store
        scores.append(rmse)
        # calculate overall RMSE
    s = 0
    for row in range(actual.shape[0]):
        for col in range(actual.shape[1]):
            s += (actual[row, col] - predicted[row, col])**2
    score = np.sqrt(s / (actual.shape[0] * actual.shape[1]))
    return score, scores
def summarize_scores(name, score, scores):
    s_scores = ' , ' .join([ ' %.1f ' % s for s in scores])
    print( ' %s: [%.3f] %s ' % (name, score, s_scores))
def forecast(model, history, n_input):
# flatten data
    data = np.array(history)
    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))
    # retrieve last observations for input data
    input_x = data[-n_input:, 0]
    # reshape into [1, n_input, 1]
    input_x = input_x.reshape((1, len(input_x), 1))
    # forecast the next week
    yhat = model.predict(input_x, verbose=0)
    # we only want the vector forecast
    yhat = yhat[0]
    return yhat
def evaluate_model(train, test, n_input):
# fit model
    model = build_model(train, n_input)
    # history is a list of weekly data
    history = [x for x in train]
    # walk-forward validation over each week
    predictions = list()
    for i in range(len(test)):
    # predict the week
        yhat_sequence = forecast(model, history, n_input)
        # store the predictions
        predictions.append(yhat_sequence)
        # get real observation and add to history for predicting the next week
        history.append(test[i, :])
    # evaluate predictions days for each week
    predictions = np.array(predictions)
    score, scores = evaluate_forecasts(test[:, :, 0], predictions)
    return score, scores
def make_3d(x,timesteps,n_features=1):
    #[sample, timesteps, features]
    return x.reshape((int(len(x)/timesteps),timesteps,n_features))
def make_2d(x):
    return x.reshape((x.shape[0]* x.shape[1] ,x.shape[2]))
def running_mean(x, N):
        cumsum = np.cumsum(np.insert(x, 0, 0))
        return (cumsum[N:] - cumsum[:-N]) / float(N)
def runningmeanFast(x, N):
    return np.convolve(x, np.ones((N,)) / N)[(N - 1):]
def make_5d(x,n_steps,n_length, n_features=1):
    #n_steps = timesteps = t = SUBSEQUENCE SAYISI
    #n_length = columns = x/n_steps = SUBSEQ UZUNLUĞU
    #[samples, timesteps, rows, cols, channels].
    #return x.reshape((x.shape[0], n_steps, 1, n_length, n_features))
    #return x.reshape((int(x.shape[0]/n_steps), n_steps, 1, n_length, n_features))
    return x.reshape((x.shape[0], n_steps, 1, n_length, n_features))
def myloss(true, pred):
    d1 = diff(true)
    d2 = diff(pred)
#    squared_difference = tf.square(d1 - d2)
    squared_difference =tf.square(tf.subtract(diff(series),diff(time)))
    loss = tf.reduce_mean(squared_difference, axis=-1)
    return loss
def denoise(x,y):
    (ca, cd) = pywt.dwt(x, "haar")
    cat = pywt.threshold(ca, y * np.std(ca), mode="soft")
    cdt = pywt.threshold(cd, y * np.std(cd), mode="soft")
    tx = pywt.idwt(cat, cdt, "haar")
    return tx
def rmspe(y_true, y_pred):
    '''
    Compute Root Mean Square Percentage Error between two arrays.
    '''
    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))

    return loss
def exp_decay(epoch, learning_rate = 0.1, decay_rate = 0.1):
    lrate = learning_rate * np.exp(-decay_rate*epoch)
    return lrate
def granger(series1,series2):
    #MULTIVARIATE
    df = pd.DataFrame([])
    df['series'] = series
    df['series2'] = time
    granger_test = sm.tsa.stattools.grangercausalitytests(df, maxlag=2, verbose=True)
    print('we want p<0.05')
    return granger_test
    #want p<0.05
def adf_test(series, signif=0.05):
    dftest = adfuller(series, autolag='AIC')
    adf = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '# Lags', '# Observations'])
    #for key, value in dftest[4].items():
    #    adf['Critical Value (%s)' % key] = value

    p = adf['p-value']
    if p <= signif:
        print(f"ADF TEST: Series is Stationary")
    else:
        print(f"ADF TEST: Series is Non-Stationary")
def kpss_test(series, **kw):
    statistic, p_value, n_lags, critical_values = kpss(series, **kw, nlags='auto')
    # Format Output
#    print(f'KPSS Statistic: {statistic}')
 #   print(f'p-value: {p_value}')
  #  print(f'num lags: {n_lags}')
   # print('Critial Values:')
    #for key, value in critical_values.items():
     #   print(f'   {key} : {value}')
    print(f'KPSS TEST: The series is {"not " if p_value < 0.05 else ""}stationary')
def log_diff(series):
    return diff(np.log(series))
def acfs(X):
    fig, ax = plt.subplots(1,2, figsize=(10,5))
    ax[0] = plot_acf(X, ax=ax[0] ,lags=15)
    ax[1] = plot_pacf(X, ax=ax[1], lags=15)
    plt.show()
def cointegration_test(df, alpha=0.05):
    """Perform Johanson's Cointegration Test and Report Summary"""
    #MULTIVARIATE
    out = coint_johansen(df.dropna(),-1,5)
    d = {'0.90':0, '0.95':1, '0.99':2}
    traces = out.lr1
    cvts = out.cvt[:, d[str(1-alpha)]]
    def adjust(val, length= 6): return str(val).ljust(length)

    # Summary
    print('Name   ::  Test Stat > C(95%)    =>   Signif  \n', '--'*20)
    for col, trace, cvt in zip(df.columns, traces, cvts):
        print(adjust(col), ':: ', adjust(round(trace,2), 9), ">", adjust(cvt, 8), ' =>  ' , trace > cvt)
def durbin_watson(model_fitted):
    out = durbin_watson(model_fitted.resid)
    for col, val in zip(df.columns, out):
        print(str(col), ':', round(val, 2))
def invert_transformation(df_train, df_forecast, second_diff=False):
    """Revert back the differencing to get the forecast to original scale."""
    df_fc = df_forecast.copy()
    columns = df_train.columns
    for col in columns:
        # Roll back 2nd Diff
        if second_diff:
            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_1d'].cumsum()
        # Roll back 1st Diff
        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()
    return df_fc
def get_stationarity(timeseries):
    # rolling statistics
    rolling_mean = timeseries.rolling(window=12).mean()
    rolling_std = timeseries.rolling(window=12).std()
    # rolling statistics plot
    plt.plot(timeseries, color='blue', label='Original')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()
def check(timeseries):
    adf_test(timeseries)
    kpss_test(timeseries)
def make_stat(timeseries):
    fig = plt.figure(figsize=(12,6))
    df = pd.Series(timeseries)
    df_log = np.log(timeseries)
    rolling_mean_exp_decay = df_log.ewm(halflife=12, min_periods=0, adjust=True).mean()
    df_log_exp_decay = df_log - rolling_mean_exp_decay
    df_log_exp_decay.dropna(inplace=True)
    print('checking log_exp_decay'.upper())
    rolling_mean = df_log_exp_decay.rolling(window=12).mean()
    rolling_std = df_log_exp_decay.rolling(window=12).std()
    # rolling statistics plot
    plt.subplot(2, 2, 1)
    plt.plot(df_log_exp_decay, color='blue', label='Original')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('LOG_EXP_DECAY')
    check(df_log_exp_decay)
    df_log_shift = df_log - df_log.shift()
    df_log_shift.dropna(inplace=True)
    print('checking log_shift'.upper())
    rolling_mean = df_log_shift.rolling(window=12).mean()
    rolling_std = df_log_shift.rolling(window=12).std()
    # rolling statistics plot
    plt.subplot(2, 2, 2)
    plt.plot(df_log_shift, color='blue', label='Original')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('LOG_SHIFT')
    check(df_log_shift)

    df_diff = timeseries.diff().dropna()
    print('checking diff1'.upper())
    rolling_mean = df_diff.rolling(window=12).mean()
    rolling_std = df_diff.rolling(window=12).std()
    # rolling statistics plot
    plt.subplot(2, 2, 4)
    plt.plot(df_diff, color='blue', label='Original')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('DIFF')
    check(df_diff)

    plt.subplot(2, 2, 3)
    rolling_mean = df.rolling(window=12).mean()
    rolling_std = df.rolling(window=12).std()
    plt.plot(df, color='blue', label='Original')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('original')
    print('checking original'.upper())
    check(df)
    return df_log_exp_decay, df_log_shift, df_diff
def diff(x):
    l = []
    for i in range(1,len(x)-1):
        l.append(x[i] - x[i-1])
    return l
##
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''CHOOSE DATA'''''''''''''''''''''''''''''''''''''''''''''''
data = np.linspace(0,1000, num=1000)
series =np.exp(2*np.sin(0.05*data))*data + np.random.normal(0, 3, data.shape) * np.tanh(data) * np.sin(data)
#series = np.sin(data*np.sin(data))+np.sin(data)+np.log(data+1) + (data/100)**2 + np.sin(data/70)*100
#np.sin(data*data*0.00001) + np.random.normal(0, 0.1, data.shape) + np.cos(0.2*data) + np.tanh(0.3*data) + np.exp(data*0.001)
#series = denoise(series,0)
#np.log(data+1)*np.sin((data+1)*0.09) #data
time = np.arange(0,len(series))
##
df = pd.read_csv('/home/taylan/Desktop/JUPYTER/BORSA/techindicdata.csv')
df_10 = df[['trend_macd','trend_sma_fast','momentum_stoch_rsi_k','momentum_stoch_rsi_d','momentum_rsi',
              'momentum_wr','volume_adi','trend_cci']]
df_10_drop = df_10.dropna()
series = df_10_drop['volume_adi'].values
series = series[len(series)%50:]
series = series[-1000:]
time = np.arange(0,len(series))
df_10_drop.plot(subplots=True)
plt.tight_layout()
plt.show()
##
np.random.seed(7)
dims = 1
step_n = 3000
step_set = [-20, 0, 20]
origin = np.zeros((1,dims))
# Simulate steps in 1D
step_shape = (step_n,dims)
steps = np.random.choice(a=step_set, size=step_shape)
#path = np.concatenate([origin, steps]).cumsum(0).reshape((-1,))
randomness = steps.cumsum(0).reshape((-1,))#+1e-1*series.reshape((10001,1))
randomness = running_mean(randomness,51) + 1000
trend = 500 * np.sin(1000*np.linspace(0,0.01, num=len(randomness))) + 1000
total = randomness + trend
horizon = 50
sondan = round(len(total),-2)
sondan = int(sondan - sondan%horizon)
sondan = round(len(total),-2)
total, trend, randomness = total[-sondan:] , trend[-sondan:] , randomness[-sondan:]
plt.plot(randomness, label = 'randomness')
plt.plot(trend, label = 'trend')
plt.plot(total, label = 'total')
plt.legend()
plt.show()
series = total
time = np.arange(0,len(series))
##
horizon = 50
ticker = 'ECILC.IS'
data = pdr.get_data_yahoo(ticker, start="2000-01-01", end="2040-04-30")
#data = data.tail(1000)
sondan = round(len(data),-2)
sondan = int(sondan - sondan%horizon)
data = data.reset_index()[-sondan:]
series = np.array(data['Close']) #diff
series = denoise(series,1)
time = np.arange(0,len(data['Date']))
##
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''TRANSFORM DATA'''''''''''''''''''''''''''''''''''''''''
#df = pd.Series(df)
#log, shift, diff = make_stat(df)
dff = np.diff(np.log(series+1)+1e-7)
series = dff[len(dff)%horizon:]
time = np.arange(0,len(series))

check(dff)
#def inverse(series):


##
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''SPLIT DATA'''''''''''''''''''''''''''''''''''''''''''''''
features = 1
horizon = 50
split_1 = len(series)-horizon*2
split_2 = len(series)-horizon

time_train = time[:split_1]
x_train = series[:split_1]
time_valid = time[split_1:split_2]
x_valid = series[split_1:split_2]
time_test = time[split_2:]
x_test = series[split_2:]
#x_valid = x_valid*0.1
time_train_val = time[:split_2]
x_train_val = series[:split_2]

plt.figure(figsize=(14,8))
plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)
plt.show()
##

sc = MinMaxScaler(feature_range=(0,1))
# define parameters
x_train_scaled = sc.fit_transform(x_train.reshape(-1,1))
x_valid_scaled = sc.transform(x_valid.reshape(-1,1))
x_test_scaled = sc.transform(x_test.reshape(-1,1))

n_input = horizon #50
n_out = horizon#len(x_valid) #50
#timestep = 100
x_train_scaled_3d = make_3d(x_train_scaled,n_out,1)
x_valid_scaled_3d = make_3d(x_valid_scaled,n_out,1)

#score , scores = evaluate_model(train,test,n_input)
train_x, train_y = to_supervised(x_train_scaled_3d, n_input, n_out)

n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]
# reshape output into [samples, timesteps, features]
train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))
##
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''SELECT MODEL'''''''''''''''''''''''''''''''''''''''''''''''
model = Sequential()
model.add(Conv1D(filters=50, kernel_size=3, activation='relu',input_shape=(n_timesteps,n_features),kernel_regularizer=keras.regularizers.l2(0.0005)))
model.add(Conv1D(filters=50, kernel_size=3, activation='relu',kernel_regularizer=keras.regularizers.l2(1e-2)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(RepeatVector(n_outputs))
model.add(LSTM(50, activation= 'relu' , return_sequences=True,kernel_regularizer=keras.regularizers.l2(1e-2)))
model.add(TimeDistributed(Dense(50, activation= 'relu' )))
model.add(TimeDistributed(Dense(1)))
model.compile(loss= 'mse' , optimizer= 'adam',  metrics=['mae','mse'])
loss_history = History()
lr_rate = LearningRateScheduler(exp_decay)
e_s = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
callbacks_list = [loss_history, e_s, lr_rate]

##
neuron = horizon*2
l2 = 0.005
model = Sequential()
model.add(tf.keras.layers.Bidirectional(LSTM(neuron*2, activation='elu', input_shape=(n_timesteps, n_features),
                                             dropout=0.1, recurrent_dropout=0.1,
                                             activity_regularizer=tf.keras.regularizers.l2(l2),
                                             recurrent_regularizer=tf.keras.regularizers.l2(l2))))
model.add(tf.keras.layers.BatchNormalization(momentum=0.95))
model.add(Dropout(rate=0.2))
model.add(RepeatVector(n_outputs))
model.add(tf.keras.layers.Bidirectional(LSTM(neuron, activation='elu', return_sequences=True,dropout=0.1, recurrent_dropout=0.1,
                                             activity_regularizer=tf.keras.regularizers.l2(l2),recurrent_regularizer=tf.keras.regularizers.l2(l2))))
model.add(Dropout(rate=0.2))
model.add(SeqSelfAttention(attention_activation='elu'))
model.add(tf.keras.layers.Bidirectional(LSTM(neuron, activation='elu', return_sequences=True)))
model.add(Dropout(rate=0.2))
model.add(TimeDistributed(Dense(neuron, activation='elu'))) #50 bi ara 100dü
model.add(TimeDistributed(Dense(1)))
model.compile(loss='mse', optimizer=keras.optimizers.Adamax(learning_rate=0.001,beta_1=0.91,beta_2=0.999), metrics=['mae','mse'])
loss_history = History()
lr_rate = LearningRateScheduler(exp_decay)
e_s = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
callbacks_list = [loss_history, e_s, lr_rate]
##
neuron = horizon
l2 = 0.005
model = Sequential()
model.add(tf.keras.layers.Bidirectional(LSTM(neuron*2, activation='elu', input_shape=(n_timesteps, n_features))))
model.add(Dropout(rate=0.2))
model.add(RepeatVector(n_outputs))
model.add(SeqSelfAttention(attention_activation='relu'))
model.add(tf.keras.layers.Bidirectional(LSTM(neuron, activation='relu', return_sequences=True)))
model.add(Dropout(rate=0.2))
model.add(TimeDistributed(Dense(neuron, activation='elu'))) #50 bi ara 100dü
model.add(TimeDistributed(Dense(1)))
model.compile(loss='mse', optimizer='adamax', metrics=['mae','mse'])
loss_history = History()
lr_rate = LearningRateScheduler(exp_decay)
e_s = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
callbacks_list = [loss_history, e_s, lr_rate]
##
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''FIT + PREDICT'''''''''''''''''''''''''''''''''''''''''''''
tf.keras.backend.clear_session()
tf.random.set_seed(7)
np.random.seed(7)
print('fit1/4')
verbose, epochs, batch_size = 1, 100, int(horizon/5)
e_s = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[callbacks_list])
'''pred(x_trainin sonu) -- > valid'''
prediction_scaled_3d = model.predict(make_3d(x_train_scaled.reshape((-1,1))[-n_input:],n_input,1))
#np.append(x_train_scaled_3d[16],x_train_scaled_3d[17])

#np.append(x_train_val[-window_size+i:],forecast[:i])[np.newaxis]
prediction_2d = sc.inverse_transform(make_2d(prediction_scaled_3d))
#score = model.evaluate(test_x, test_y, batch_size=10)
#plt.plot(history.history['loss'])
#plt.show()
plt.figure(figsize=(12,6))
plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)
plot_series(time_valid,prediction_2d)
plt.show()
#print(r2_score(x_valid,prediction_2d))
print('valid',round(r2_score(x_valid,prediction_2d),3),'corr_valid',round(pd.Series(x_valid).corr(pd.Series(prediction_2d.reshape(-1,))),3),'diff_valid',
    round(pd.Series(diff(x_valid)).corr(pd.Series(diff(prediction_2d.reshape(-1,)))),3))

##
'''fit(x_train, y_train) , pred(x_validin sonu)'''
tf.keras.backend.clear_session()
tf.random.set_seed(7)
np.random.seed(7)
print('fit2/4')
'''x_train_scaled = sc.fit_transform(x_train.reshape(-1,1))
x_val_scaled = sc.transform(x_valid.reshape(-1,1))'''
#timestep = 100
x_train_scaled_3d = make_3d(x_train_scaled,n_out,1)
x_test_scaled_3d = make_3d(x_test_scaled,n_out,1)
#score , scores = evaluate_model(train,test,n_input)
train_val_x, train_val_y = to_supervised(x_train_scaled_3d, n_input, n_out)
#test_x, test_y = to_supervised(x_test_scaled_3d, n_input, n_out)
hhistory = model.fit(train_x,train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[callbacks_list])
pprediction_scaled_3d = model.predict(make_3d(np.append(x_train_scaled_3d[17],x_valid_scaled_3d[0]).reshape((-1,1))[-n_input:],n_input,1))
pprediction_2d = sc.inverse_transform(make_2d(pprediction_scaled_3d))
plt.figure(figsize=(12,6))
plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)
plot_series(time_valid,prediction_2d)
plot_series(time_test,pprediction_2d)
plt.show()
#print(tf.keras.metrics.mean_squared_error(x_valid, prediction_2d).numpy())
print('valid',round(r2_score(x_valid,prediction_2d),3),'test',round(r2_score(x_test,pprediction_2d),3),'\ncorr_valid',
    round(pd.Series(x_valid).corr(pd.Series(prediction_2d.reshape(-1,))),3),'corr_test',round(pd.Series(x_valid).corr(
    pd.Series(pprediction_2d.reshape(-1,))),3),'\ndiff_valid',round(pd.Series(diff(x_valid)).corr(pd.Series(diff(prediction_2d.reshape((-1,))))),3),
      'diff_test',round(pd.Series(diff(x_valid)).corr(pd.Series(diff(pprediction_2d.reshape((-1,))))),3))
##
'''fit(train, valid) , pred(x_validin sonu)'''
tf.keras.backend.clear_session()
tf.random.set_seed(7)
np.random.seed(7)
print('fit3/4')
x_train_val_scaled = sc.fit_transform(x_train_val.reshape(-1,1))
x_test_scaled = sc.transform(x_test.reshape(-1,1))
#timestep = 100
x_train_val_scaled_3d = make_3d(x_train_val_scaled,n_out,1)
x_test_scaled_3d = make_3d(x_test_scaled,n_out,1)

#score , scores = evaluate_model(train,test,n_input)
train_val_x, train_val_y = to_supervised(x_train_val_scaled_3d, n_input, n_out)
#test_x, test_y = to_supervised(x_test_scaled_3d, n_input, n_out)

hhistory = model.fit(train_val_x,train_val_y, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[callbacks_list])
ppprediction_scaled_3d = model.predict(make_3d(x_train_val_scaled_3d[-1],horizon,1))
ppprediction_2d = sc.inverse_transform(make_2d(ppprediction_scaled_3d))
plt.figure(figsize=(12,6))
plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)
plot_series(time_valid,prediction_2d)
plot_series(time_test,pprediction_2d)
plot_series(time_test,ppprediction_2d)
plt.show()

#print(tf.keras.metrics.mean_squared_error(x_valid, prediction_2d).numpy())
print('valid',round(r2_score(x_valid,prediction_2d),3),'test',round(r2_score(x_test,ppprediction_2d),3),
      '\ncorr_valid',round(pd.Series(x_valid).corr(pd.Series(prediction_2d.reshape(-1,))),3),'corr_test',round(pd.Series(x_valid).corr(
    pd.Series(ppprediction_2d.reshape(-1,))),3),
      '\ndiff_valid',round(pd.Series(diff(x_valid)).corr(pd.Series(diff(prediction_2d.reshape((-1,))))),3),
      'diff_test',round(pd.Series(diff(x_valid)).corr(pd.Series(diff(ppprediction_2d.reshape((-1,))))),3))

##
'''EXTRAPOLATE'''
pppprediction_scaled_3d = model.predict(make_3d(x_test_scaled,horizon,1))
pppprediction_2d = sc.inverse_transform(make_2d(pppprediction_scaled_3d))
time_new = np.arange(time_test[-1],time_test[-1]+n_out)

'''plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)'''
N = 31
a = N/2
b = -(N/2-1)
mov = running_mean(series,N)
time_mov = np.arange((int(len(series)-len(mov))/2),int(len(series)-int(len(series)-len(mov))/2))
plt.figure(figsize=(12,6))
plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)
plot_series(time_mov,mov)
plot_series(time_valid,prediction_2d)
plot_series(time_test,pprediction_2d)
plot_series(time_test,ppprediction_2d)
plot_series(time_new,pppprediction_2d)
#plot_series(time_new,pppprediction_2d)
plt.show()

##

'''LSTM'''
'''valid 0.9623810094619425 test 0.8124706639451762 
corr_valid 0.9976038386191789 corr_test -0.6696176051666543 
diff_valid 0.9727168617252929 diff_test -0.37821635851459967'''

#momentum, decay, batch normal, weight decay

A more complex dataset requires less regularization so test smaller weight decay values, such as 10−4 , 10−5 , 10−6 , 0.
A shallow architecture requires more regularization so test larger weight decay values, such as 10−2 , 10−3 , 10−4 .
0.9, 0.99


##
'''CONVLSTM IN PROGRESS'''
'''x_train_scaled_5d = make_3d(x_train_scaled,n_out,1)
x_valid_scaled_5d = make_3d(x_valid_scaled,n_out,1)
train_x, train_y = to_supervised(x_train_scaled_5d, n_input, n_out)
train_x = train_x.reshape((800,2,1,25,1))
# define parameters
verbose, epochs, batch_size = 0, 20, 16
n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]
# reshape into subsequences [samples, timesteps, rows, cols, channels]
#train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))
# reshape output into [samples, timesteps, features]
train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))
# define model
model = Sequential()
model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation= 'relu' ,input_shape=(2, 1, 25, n_features)))
model.add(Flatten())
model.add(RepeatVector(n_outputs))
model.add(LSTM(200, activation= 'relu' , return_sequences=True))
model.add(TimeDistributed(Dense(100, activation= 'relu' )))
model.add(TimeDistributed(Dense(1)))
model.compile(loss= 'mse' , optimizer= 'adam' , metrics=['mae','mse'] )
tf.keras.backend.clear_session()
tf.random.set_seed(7)
np.random.seed(7)
print('fit0/4')
verbose, epochs, batch_size = 1, 1, 10
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[callback])
prediction_scaled_5d = model.predict(make_3d(x_train_scaled.reshape((-1,1))[-n_input:],n_input,1).reshape((-1,2,1,25,1)))
#np.append(x_train_scaled_3d[16],x_train_scaled_3d[17])

#np.append(x_train_val[-window_size+i:],forecast[:i])[np.newaxis]
prediction_2d = sc.inverse_transform(make_2d(prediction_scaled_5d))
#score = model.evaluate(test_x, test_y, batch_size=10)
#plt.plot(history.history['loss'])
#plt.show()
plt.figure(figsize=(12,6))
plot_series(time_train,x_train)
plot_series(time_valid,x_valid)
plot_series(time_test,x_test)
plot_series(time_valid,prediction_2d)
plt.show()
tf.keras.metrics.mean_squared_error(x_valid, prediction_2d).numpy()
#print(r2_score(x_valid,prediction_2d))
print('valid',r2_score(x_valid,prediction_2d),'behavior_valid',
    pd.Series(x_valid).corr(pd.Series(prediction_2d.reshape(-1,))))

'''
##
n = 100#input norons
l = 50#output norons
m =np.ceil(np.sqrt(0.43*l*n+0.12*l**2+2.54*n+0.77*l+0.35)+0.51)
##


